{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Anomaly Detection using Unsupervised Techniques","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n#for data preprocessing\nfrom sklearn.decomposition import PCA\n\n#for modeling\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\n\n#filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/healthcare-providers-data/Healthcare Providers.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DropCols = ['index', 'National Provider Identifier',\n       'Last Name/Organization Name of the Provider',\n       'First Name of the Provider', 'Middle Initial of the Provider','Street Address 1 of the Provider',\n       'Street Address 2 of the Provider','Zip Code of the Provider',\"HCPCS Code\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(DropCols, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Entity Type of the Provider\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Average Submitted Charge Amount\"].dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Average Submitted Charge Amount\"].dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cleaning\n\ndef RemoveComma(x):\n    return x.replace(\",\",\"\")\n\ndf[\"Average Medicare Allowed Amount\"] = pd.to_numeric(df[\"Average Medicare Allowed Amount\"].apply(lambda x: RemoveComma(x)),\n                                                             errors= \"ignore\")\ndf[\"Average Submitted Charge Amount\"] = pd.to_numeric(df[\"Average Submitted Charge Amount\"].apply(lambda x: RemoveComma(x)),\n                                                       errors = \"ignore\")\ndf[\"Average Medicare Payment Amount\"] = pd.to_numeric(df[\"Average Medicare Payment Amount\"].apply(lambda x: RemoveComma(x)),\n                                                       errors = \"ignore\")\ndf[\"Average Medicare Standardized Amount\"] = pd.to_numeric(df[\"Average Medicare Standardized Amount\"].apply(lambda x: RemoveComma(x)),\n                                                             errors = \"ignore\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import category_encoders as ce\nfrom sklearn.preprocessing import StandardScaler\n\ndef RemoveComma(x):\n    return x.replace(\",\",\"\")\n\ndef Preprocessing(data):\n    \n    \n    #1.Imputing Missing Values\n\n    data[\"Credentials of the Provider\"] = data[\"Credentials of the Provider\"].fillna(data[\"Credentials of the Provider\"].mode()[0])\n    data[\"Gender of the Provider\"] = data[\"Gender of the Provider\"].fillna(data[\"Gender of the Provider\"].mode()[0])\n    \n\n   #2.Binary Encoding.\n\n    \n    BEcols = [var for var in data.columns if data[var].dtype == \"O\"]\n    \n    for col in BEcols:\n        encoder = ce.BinaryEncoder(cols = [col])\n        dfbin = encoder.fit_transform(data[col])\n        data = pd.concat([data,dfbin], axis = 1)\n        del data[col]\n\n    #3. One-Hot-Encoding\n\n#     data = pd.get_dummies(data,drop_first = True)\n    \n \n    #4. Standardization\n \n    data_columns = data.columns\n    std = StandardScaler()\n    data = std.fit_transform(data)\n    data = pd.DataFrame(data, columns = data_columns)\n    \n    return data\n\n\ndf = Preprocessing(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(n_estimators=300, max_samples='auto', \n                        contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, \n                        behaviour='deprecated', verbose=1, warm_start=False, random_state=2020)\nmodel.fit(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = model.predict(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y[Y == 1] = 0\nY[Y == -1] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(2)\nx_pca = pca.fit_transform(df)\nx_pca = pd.DataFrame(x_pca)\nx_pca.columns = ['pc1', 'pc2']\n\nplt.figure(figsize=(12,8))\nplt.title('Sample Distribution on First 2 PCAs by Class Color')\nplt.scatter(x_pca['pc1'], x_pca['pc2'], alpha = .6, c=Y)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(3)\nx_pca = pca.fit_transform(df)\nx_pca = pd.DataFrame(x_pca)\nx_pca.columns = ['pc1', 'pc2', 'pc3']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_pca[\"Label\"] = Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x= x_pca['pc1'],\n    y= x_pca['pc2'],\n    z= x_pca['pc3'],\n    mode='markers',\n     marker=dict(\n        color = x_pca['Label'], \n        size= 10,\n        line=dict(\n            color= x_pca['Label'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndt = [trace1]\n\nlayout = go.Layout(\n    title = 'Character vs Gender vs Alive or not',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    ),\n    scene = dict(\n            xaxis = dict(title  = 'PC1'),\n            yaxis = dict(title  = 'PC2'),\n            zaxis = dict(title  = 'PC3')\n        )\n)\n\nfig = go.Figure(data = dt, layout = layout)\npy.iplot(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto Encoders","metadata":{}},{"cell_type":"code","source":"pip install pyod","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyod.models.auto_encoder import AutoEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf1 = AutoEncoder(hidden_neurons =[15, 10, 6, 2, 2, 6, 10, 15], epochs = 26, contamination = .002)\nclf1.fit(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = clf1.decision_scores_\ny_scores = clf1.decision_function(df)  #map all points to one a line\ny_scores = pd.Series(y_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,7))\nplt.axvline(18, color = 'b', alpha = .9)\nplt.hist(y_scores, bins=1000)  \nplt.title(\"Histogram for Model Clf1 Anomaly Scores\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Points above 18 are considered as anomalies.","metadata":{}},{"cell_type":"markdown","source":"# Thanks for reading! Kindly share your approach on Anomaly Detection.","metadata":{"trusted":true}}]}